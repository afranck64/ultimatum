{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: hunga-bunga in /home/franck/anaconda3/envs/thesis/lib/python3.6/site-packages (0.1)\r\n",
      "Requirement already satisfied: numpy in /home/franck/anaconda3/envs/thesis/lib/python3.6/site-packages (from hunga-bunga) (1.16.1)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install hunga-bunga"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from IPython.display import display\n",
    "from sklearn.model_selection import train_test_split, cross_validate\n",
    "from sklearn.metrics import r2_score\n",
    "import seaborn as sb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.utils.preprocessing import df_to_xy\n",
    "#fix rnd seed\n",
    "#np.random.seed(7)\n",
    "\n",
    "# Read and sanitize the data\n",
    "df = pd.read_excel(\"../data/t00/data.xls\")\n",
    "\n",
    "x, y = df_to_xy(df, fuse_risk=True, normalize=True, df_min=df.min(), df_max=df.max())\n",
    "xTrain, xTest, yTrain, yTest = train_test_split(x, y, test_size = 2/5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/15 [00:00<?, ?it/s]\u001b[ATraceback (most recent call last):\n",
      "  File \"/home/franck/anaconda3/envs/thesis/lib/python3.6/site-packages/hunga_bunga/core.py\", line 105, in main_loop\n",
      "    clf_search.fit(x, y)\n",
      "  File \"/home/franck/anaconda3/envs/thesis/lib/python3.6/site-packages/sklearn/model_selection/_search.py\", line 605, in fit\n",
      "    cv = check_cv(self.cv, y, classifier=is_classifier(estimator))\n",
      "  File \"/home/franck/anaconda3/envs/thesis/lib/python3.6/site-packages/sklearn/model_selection/_split.py\", line 1993, in check_cv\n",
      "    return _CVIterableWrapper(cv)\n",
      "  File \"/home/franck/anaconda3/envs/thesis/lib/python3.6/site-packages/sklearn/model_selection/_split.py\", line 1891, in __init__\n",
      "    self.cv = list(cv)\n",
      "  File \"/home/franck/anaconda3/envs/thesis/lib/python3.6/site-packages/hunga_bunga/core.py\", line 78, in cv_clf\n",
      "    for train_inds, valid_inds in sss_obj: yield (upsample_indices_clf(train_inds, y[train_inds]), valid_inds)\n",
      "  File \"/home/franck/anaconda3/envs/thesis/lib/python3.6/site-packages/sklearn/model_selection/_split.py\", line 1323, in split\n",
      "    for train, test in self._iter_indices(X, y, groups):\n",
      "  File \"/home/franck/anaconda3/envs/thesis/lib/python3.6/site-packages/sklearn/model_selection/_split.py\", line 1636, in _iter_indices\n",
      "    raise ValueError(\"The least populated class in y has only 1\"\n",
      "ValueError: The least populated class in y has only 1 member, which is too few. The minimum number of groups for any class cannot be less than 2.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/franck/anaconda3/envs/thesis/lib/python3.6/site-packages/hunga_bunga/core.py\", line 105, in main_loop\n",
      "    clf_search.fit(x, y)\n",
      "  File \"/home/franck/anaconda3/envs/thesis/lib/python3.6/site-packages/sklearn/model_selection/_search.py\", line 605, in fit\n",
      "    cv = check_cv(self.cv, y, classifier=is_classifier(estimator))\n",
      "  File \"/home/franck/anaconda3/envs/thesis/lib/python3.6/site-packages/sklearn/model_selection/_split.py\", line 1993, in check_cv\n",
      "    return _CVIterableWrapper(cv)\n",
      "  File \"/home/franck/anaconda3/envs/thesis/lib/python3.6/site-packages/sklearn/model_selection/_split.py\", line 1891, in __init__\n",
      "    self.cv = list(cv)\n",
      "  File \"/home/franck/anaconda3/envs/thesis/lib/python3.6/site-packages/hunga_bunga/core.py\", line 78, in cv_clf\n",
      "    for train_inds, valid_inds in sss_obj: yield (upsample_indices_clf(train_inds, y[train_inds]), valid_inds)\n",
      "  File \"/home/franck/anaconda3/envs/thesis/lib/python3.6/site-packages/sklearn/model_selection/_split.py\", line 1323, in split\n",
      "    for train, test in self._iter_indices(X, y, groups):\n",
      "  File \"/home/franck/anaconda3/envs/thesis/lib/python3.6/site-packages/sklearn/model_selection/_split.py\", line 1636, in _iter_indices\n",
      "    raise ValueError(\"The least populated class in y has only 1\"\n",
      "ValueError: The least populated class in y has only 1 member, which is too few. The minimum number of groups for any class cannot be less than 2.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/franck/anaconda3/envs/thesis/lib/python3.6/site-packages/hunga_bunga/core.py\", line 105, in main_loop\n",
      "    clf_search.fit(x, y)\n",
      "  File \"/home/franck/anaconda3/envs/thesis/lib/python3.6/site-packages/sklearn/model_selection/_search.py\", line 605, in fit\n",
      "    cv = check_cv(self.cv, y, classifier=is_classifier(estimator))\n",
      "  File \"/home/franck/anaconda3/envs/thesis/lib/python3.6/site-packages/sklearn/model_selection/_split.py\", line 1993, in check_cv\n",
      "    return _CVIterableWrapper(cv)\n",
      "  File \"/home/franck/anaconda3/envs/thesis/lib/python3.6/site-packages/sklearn/model_selection/_split.py\", line 1891, in __init__\n",
      "    self.cv = list(cv)\n",
      "  File \"/home/franck/anaconda3/envs/thesis/lib/python3.6/site-packages/hunga_bunga/core.py\", line 78, in cv_clf\n",
      "    for train_inds, valid_inds in sss_obj: yield (upsample_indices_clf(train_inds, y[train_inds]), valid_inds)\n",
      "  File \"/home/franck/anaconda3/envs/thesis/lib/python3.6/site-packages/sklearn/model_selection/_split.py\", line 1323, in split\n",
      "    for train, test in self._iter_indices(X, y, groups):\n",
      "  File \"/home/franck/anaconda3/envs/thesis/lib/python3.6/site-packages/sklearn/model_selection/_split.py\", line 1636, in _iter_indices\n",
      "    raise ValueError(\"The least populated class in y has only 1\"\n",
      "ValueError: The least populated class in y has only 1 member, which is too few. The minimum number of groups for any class cannot be less than 2.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/franck/anaconda3/envs/thesis/lib/python3.6/site-packages/hunga_bunga/core.py\", line 105, in main_loop\n",
      "    clf_search.fit(x, y)\n",
      "  File \"/home/franck/anaconda3/envs/thesis/lib/python3.6/site-packages/sklearn/model_selection/_search.py\", line 605, in fit\n",
      "    cv = check_cv(self.cv, y, classifier=is_classifier(estimator))\n",
      "  File \"/home/franck/anaconda3/envs/thesis/lib/python3.6/site-packages/sklearn/model_selection/_split.py\", line 1993, in check_cv\n",
      "    return _CVIterableWrapper(cv)\n",
      "  File \"/home/franck/anaconda3/envs/thesis/lib/python3.6/site-packages/sklearn/model_selection/_split.py\", line 1891, in __init__\n",
      "    self.cv = list(cv)\n",
      "  File \"/home/franck/anaconda3/envs/thesis/lib/python3.6/site-packages/hunga_bunga/core.py\", line 78, in cv_clf\n",
      "    for train_inds, valid_inds in sss_obj: yield (upsample_indices_clf(train_inds, y[train_inds]), valid_inds)\n",
      "  File \"/home/franck/anaconda3/envs/thesis/lib/python3.6/site-packages/sklearn/model_selection/_split.py\", line 1323, in split\n",
      "    for train, test in self._iter_indices(X, y, groups):\n",
      "  File \"/home/franck/anaconda3/envs/thesis/lib/python3.6/site-packages/sklearn/model_selection/_split.py\", line 1636, in _iter_indices\n",
      "    raise ValueError(\"The least populated class in y has only 1\"\n",
      "ValueError: The least populated class in y has only 1 member, which is too few. The minimum number of groups for any class cannot be less than 2.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/franck/anaconda3/envs/thesis/lib/python3.6/site-packages/hunga_bunga/core.py\", line 105, in main_loop\n",
      "    clf_search.fit(x, y)\n",
      "  File \"/home/franck/anaconda3/envs/thesis/lib/python3.6/site-packages/sklearn/model_selection/_search.py\", line 605, in fit\n",
      "    cv = check_cv(self.cv, y, classifier=is_classifier(estimator))\n",
      "  File \"/home/franck/anaconda3/envs/thesis/lib/python3.6/site-packages/sklearn/model_selection/_split.py\", line 1993, in check_cv\n",
      "    return _CVIterableWrapper(cv)\n",
      "  File \"/home/franck/anaconda3/envs/thesis/lib/python3.6/site-packages/sklearn/model_selection/_split.py\", line 1891, in __init__\n",
      "    self.cv = list(cv)\n",
      "  File \"/home/franck/anaconda3/envs/thesis/lib/python3.6/site-packages/hunga_bunga/core.py\", line 78, in cv_clf\n",
      "    for train_inds, valid_inds in sss_obj: yield (upsample_indices_clf(train_inds, y[train_inds]), valid_inds)\n",
      "  File \"/home/franck/anaconda3/envs/thesis/lib/python3.6/site-packages/sklearn/model_selection/_split.py\", line 1323, in split\n",
      "    for train, test in self._iter_indices(X, y, groups):\n",
      "  File \"/home/franck/anaconda3/envs/thesis/lib/python3.6/site-packages/sklearn/model_selection/_split.py\", line 1636, in _iter_indices\n",
      "    raise ValueError(\"The least populated class in y has only 1\"\n",
      "ValueError: The least populated class in y has only 1 member, which is too few. The minimum number of groups for any class cannot be less than 2.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/franck/anaconda3/envs/thesis/lib/python3.6/site-packages/hunga_bunga/core.py\", line 105, in main_loop\n",
      "    clf_search.fit(x, y)\n",
      "  File \"/home/franck/anaconda3/envs/thesis/lib/python3.6/site-packages/sklearn/model_selection/_search.py\", line 605, in fit\n",
      "    cv = check_cv(self.cv, y, classifier=is_classifier(estimator))\n",
      "  File \"/home/franck/anaconda3/envs/thesis/lib/python3.6/site-packages/sklearn/model_selection/_split.py\", line 1993, in check_cv\n",
      "    return _CVIterableWrapper(cv)\n",
      "  File \"/home/franck/anaconda3/envs/thesis/lib/python3.6/site-packages/sklearn/model_selection/_split.py\", line 1891, in __init__\n",
      "    self.cv = list(cv)\n",
      "  File \"/home/franck/anaconda3/envs/thesis/lib/python3.6/site-packages/hunga_bunga/core.py\", line 78, in cv_clf\n",
      "    for train_inds, valid_inds in sss_obj: yield (upsample_indices_clf(train_inds, y[train_inds]), valid_inds)\n",
      "  File \"/home/franck/anaconda3/envs/thesis/lib/python3.6/site-packages/sklearn/model_selection/_split.py\", line 1323, in split\n",
      "    for train, test in self._iter_indices(X, y, groups):\n",
      "  File \"/home/franck/anaconda3/envs/thesis/lib/python3.6/site-packages/sklearn/model_selection/_split.py\", line 1636, in _iter_indices\n",
      "    raise ValueError(\"The least populated class in y has only 1\"\n",
      "ValueError: The least populated class in y has only 1 member, which is too few. The minimum number of groups for any class cannot be less than 2.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/franck/anaconda3/envs/thesis/lib/python3.6/site-packages/hunga_bunga/core.py\", line 105, in main_loop\n",
      "    clf_search.fit(x, y)\n",
      "  File \"/home/franck/anaconda3/envs/thesis/lib/python3.6/site-packages/sklearn/model_selection/_search.py\", line 605, in fit\n",
      "    cv = check_cv(self.cv, y, classifier=is_classifier(estimator))\n",
      "  File \"/home/franck/anaconda3/envs/thesis/lib/python3.6/site-packages/sklearn/model_selection/_split.py\", line 1993, in check_cv\n",
      "    return _CVIterableWrapper(cv)\n",
      "  File \"/home/franck/anaconda3/envs/thesis/lib/python3.6/site-packages/sklearn/model_selection/_split.py\", line 1891, in __init__\n",
      "    self.cv = list(cv)\n",
      "  File \"/home/franck/anaconda3/envs/thesis/lib/python3.6/site-packages/hunga_bunga/core.py\", line 78, in cv_clf\n",
      "    for train_inds, valid_inds in sss_obj: yield (upsample_indices_clf(train_inds, y[train_inds]), valid_inds)\n",
      "  File \"/home/franck/anaconda3/envs/thesis/lib/python3.6/site-packages/sklearn/model_selection/_split.py\", line 1323, in split\n",
      "    for train, test in self._iter_indices(X, y, groups):\n",
      "  File \"/home/franck/anaconda3/envs/thesis/lib/python3.6/site-packages/sklearn/model_selection/_split.py\", line 1636, in _iter_indices\n",
      "    raise ValueError(\"The least populated class in y has only 1\"\n",
      "ValueError: The least populated class in y has only 1 member, which is too few. The minimum number of groups for any class cannot be less than 2.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/franck/anaconda3/envs/thesis/lib/python3.6/site-packages/hunga_bunga/core.py\", line 105, in main_loop\n",
      "    clf_search.fit(x, y)\n",
      "  File \"/home/franck/anaconda3/envs/thesis/lib/python3.6/site-packages/sklearn/model_selection/_search.py\", line 605, in fit\n",
      "    cv = check_cv(self.cv, y, classifier=is_classifier(estimator))\n",
      "  File \"/home/franck/anaconda3/envs/thesis/lib/python3.6/site-packages/sklearn/model_selection/_split.py\", line 1993, in check_cv\n",
      "    return _CVIterableWrapper(cv)\n",
      "  File \"/home/franck/anaconda3/envs/thesis/lib/python3.6/site-packages/sklearn/model_selection/_split.py\", line 1891, in __init__\n",
      "    self.cv = list(cv)\n",
      "  File \"/home/franck/anaconda3/envs/thesis/lib/python3.6/site-packages/hunga_bunga/core.py\", line 78, in cv_clf\n",
      "    for train_inds, valid_inds in sss_obj: yield (upsample_indices_clf(train_inds, y[train_inds]), valid_inds)\n",
      "  File \"/home/franck/anaconda3/envs/thesis/lib/python3.6/site-packages/sklearn/model_selection/_split.py\", line 1323, in split\n",
      "    for train, test in self._iter_indices(X, y, groups):\n",
      "  File \"/home/franck/anaconda3/envs/thesis/lib/python3.6/site-packages/sklearn/model_selection/_split.py\", line 1636, in _iter_indices\n",
      "    raise ValueError(\"The least populated class in y has only 1\"\n",
      "ValueError: The least populated class in y has only 1 member, which is too few. The minimum number of groups for any class cannot be less than 2.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/franck/anaconda3/envs/thesis/lib/python3.6/site-packages/hunga_bunga/core.py\", line 105, in main_loop\n",
      "    clf_search.fit(x, y)\n",
      "  File \"/home/franck/anaconda3/envs/thesis/lib/python3.6/site-packages/sklearn/model_selection/_search.py\", line 605, in fit\n",
      "    cv = check_cv(self.cv, y, classifier=is_classifier(estimator))\n",
      "  File \"/home/franck/anaconda3/envs/thesis/lib/python3.6/site-packages/sklearn/model_selection/_split.py\", line 1993, in check_cv\n",
      "    return _CVIterableWrapper(cv)\n",
      "  File \"/home/franck/anaconda3/envs/thesis/lib/python3.6/site-packages/sklearn/model_selection/_split.py\", line 1891, in __init__\n",
      "    self.cv = list(cv)\n",
      "  File \"/home/franck/anaconda3/envs/thesis/lib/python3.6/site-packages/hunga_bunga/core.py\", line 78, in cv_clf\n",
      "    for train_inds, valid_inds in sss_obj: yield (upsample_indices_clf(train_inds, y[train_inds]), valid_inds)\n",
      "  File \"/home/franck/anaconda3/envs/thesis/lib/python3.6/site-packages/sklearn/model_selection/_split.py\", line 1323, in split\n",
      "    for train, test in self._iter_indices(X, y, groups):\n",
      "  File \"/home/franck/anaconda3/envs/thesis/lib/python3.6/site-packages/sklearn/model_selection/_split.py\", line 1636, in _iter_indices\n",
      "    raise ValueError(\"The least populated class in y has only 1\"\n",
      "ValueError: The least populated class in y has only 1 member, which is too few. The minimum number of groups for any class cannot be less than 2.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/franck/anaconda3/envs/thesis/lib/python3.6/site-packages/hunga_bunga/core.py\", line 105, in main_loop\n",
      "    clf_search.fit(x, y)\n",
      "  File \"/home/franck/anaconda3/envs/thesis/lib/python3.6/site-packages/sklearn/model_selection/_search.py\", line 605, in fit\n",
      "    cv = check_cv(self.cv, y, classifier=is_classifier(estimator))\n",
      "  File \"/home/franck/anaconda3/envs/thesis/lib/python3.6/site-packages/sklearn/model_selection/_split.py\", line 1993, in check_cv\n",
      "    return _CVIterableWrapper(cv)\n",
      "  File \"/home/franck/anaconda3/envs/thesis/lib/python3.6/site-packages/sklearn/model_selection/_split.py\", line 1891, in __init__\n",
      "    self.cv = list(cv)\n",
      "  File \"/home/franck/anaconda3/envs/thesis/lib/python3.6/site-packages/hunga_bunga/core.py\", line 78, in cv_clf\n",
      "    for train_inds, valid_inds in sss_obj: yield (upsample_indices_clf(train_inds, y[train_inds]), valid_inds)\n",
      "  File \"/home/franck/anaconda3/envs/thesis/lib/python3.6/site-packages/sklearn/model_selection/_split.py\", line 1323, in split\n",
      "    for train, test in self._iter_indices(X, y, groups):\n",
      "  File \"/home/franck/anaconda3/envs/thesis/lib/python3.6/site-packages/sklearn/model_selection/_split.py\", line 1636, in _iter_indices\n",
      "    raise ValueError(\"The least populated class in y has only 1\"\n",
      "ValueError: The least populated class in y has only 1 member, which is too few. The minimum number of groups for any class cannot be less than 2.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/franck/anaconda3/envs/thesis/lib/python3.6/site-packages/hunga_bunga/core.py\", line 105, in main_loop\n",
      "    clf_search.fit(x, y)\n",
      "  File \"/home/franck/anaconda3/envs/thesis/lib/python3.6/site-packages/sklearn/model_selection/_search.py\", line 605, in fit\n",
      "    cv = check_cv(self.cv, y, classifier=is_classifier(estimator))\n",
      "  File \"/home/franck/anaconda3/envs/thesis/lib/python3.6/site-packages/sklearn/model_selection/_split.py\", line 1993, in check_cv\n",
      "    return _CVIterableWrapper(cv)\n",
      "  File \"/home/franck/anaconda3/envs/thesis/lib/python3.6/site-packages/sklearn/model_selection/_split.py\", line 1891, in __init__\n",
      "    self.cv = list(cv)\n",
      "  File \"/home/franck/anaconda3/envs/thesis/lib/python3.6/site-packages/hunga_bunga/core.py\", line 78, in cv_clf\n",
      "    for train_inds, valid_inds in sss_obj: yield (upsample_indices_clf(train_inds, y[train_inds]), valid_inds)\n",
      "  File \"/home/franck/anaconda3/envs/thesis/lib/python3.6/site-packages/sklearn/model_selection/_split.py\", line 1323, in split\n",
      "    for train, test in self._iter_indices(X, y, groups):\n",
      "  File \"/home/franck/anaconda3/envs/thesis/lib/python3.6/site-packages/sklearn/model_selection/_split.py\", line 1636, in _iter_indices\n",
      "    raise ValueError(\"The least populated class in y has only 1\"\n",
      "ValueError: The least populated class in y has only 1 member, which is too few. The minimum number of groups for any class cannot be less than 2.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/franck/anaconda3/envs/thesis/lib/python3.6/site-packages/hunga_bunga/core.py\", line 105, in main_loop\n",
      "    clf_search.fit(x, y)\n",
      "  File \"/home/franck/anaconda3/envs/thesis/lib/python3.6/site-packages/sklearn/model_selection/_search.py\", line 605, in fit\n",
      "    cv = check_cv(self.cv, y, classifier=is_classifier(estimator))\n",
      "  File \"/home/franck/anaconda3/envs/thesis/lib/python3.6/site-packages/sklearn/model_selection/_split.py\", line 1993, in check_cv\n",
      "    return _CVIterableWrapper(cv)\n",
      "  File \"/home/franck/anaconda3/envs/thesis/lib/python3.6/site-packages/sklearn/model_selection/_split.py\", line 1891, in __init__\n",
      "    self.cv = list(cv)\n",
      "  File \"/home/franck/anaconda3/envs/thesis/lib/python3.6/site-packages/hunga_bunga/core.py\", line 78, in cv_clf\n",
      "    for train_inds, valid_inds in sss_obj: yield (upsample_indices_clf(train_inds, y[train_inds]), valid_inds)\n",
      "  File \"/home/franck/anaconda3/envs/thesis/lib/python3.6/site-packages/sklearn/model_selection/_split.py\", line 1323, in split\n",
      "    for train, test in self._iter_indices(X, y, groups):\n",
      "  File \"/home/franck/anaconda3/envs/thesis/lib/python3.6/site-packages/sklearn/model_selection/_split.py\", line 1636, in _iter_indices\n",
      "    raise ValueError(\"The least populated class in y has only 1\"\n",
      "ValueError: The least populated class in y has only 1 member, which is too few. The minimum number of groups for any class cannot be less than 2.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/franck/anaconda3/envs/thesis/lib/python3.6/site-packages/hunga_bunga/core.py\", line 102, in main_loop\n",
      "    elif clf_Klass in TREE_N_ENSEMBLE_MODELS: parameters['max_features'] = [v for v in parameters['max_features'] if v is None or type(v)==str or v<=num_features]\n",
      "KeyError: 'max_features'\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/franck/anaconda3/envs/thesis/lib/python3.6/site-packages/hunga_bunga/core.py\", line 102, in main_loop\n",
      "    elif clf_Klass in TREE_N_ENSEMBLE_MODELS: parameters['max_features'] = [v for v in parameters['max_features'] if v is None or type(v)==str or v<=num_features]\n",
      "KeyError: 'max_features'\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/franck/anaconda3/envs/thesis/lib/python3.6/site-packages/hunga_bunga/core.py\", line 102, in main_loop\n",
      "    elif clf_Klass in TREE_N_ENSEMBLE_MODELS: parameters['max_features'] = [v for v in parameters['max_features'] if v is None or type(v)==str or v<=num_features]\n",
      "KeyError: 'max_features'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:00<00:00, 392.35it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scoring criteria: accuracy\n",
      "--------------- model 1/15 ---------------\n",
      "SGDClassifier\n",
      "--------------- model 2/15 ---------------\n",
      "LogisticRegression\n",
      "--------------- model 3/15 ---------------\n",
      "Perceptron\n",
      "--------------- model 4/15 ---------------\n",
      "PassiveAggressiveClassifier\n",
      "--------------- model 5/15 ---------------\n",
      "MLPClassifier\n",
      "--------------- model 6/15 ---------------\n",
      "KMeans\n",
      "--------------- model 7/15 ---------------\n",
      "KNeighborsClassifier\n",
      "--------------- model 8/15 ---------------\n",
      "NearestCentroid\n",
      "--------------- model 9/15 ---------------\n",
      "RadiusNeighborsClassifier\n",
      "--------------- model 10/15 ---------------\n",
      "SVC\n",
      "--------------- model 11/15 ---------------\n",
      "NuSVC\n",
      "--------------- model 12/15 ---------------\n",
      "LinearSVC\n",
      "--------------- model 13/15 ---------------\n",
      "RandomForestClassifier\n",
      "--------------- model 14/15 ---------------\n",
      "DecisionTreeClassifier\n",
      "--------------- model 15/15 ---------------\n",
      "ExtraTreesClassifier\n",
      "============================================================\n",
      "Model                          accuracy    Time/clf (s)\n",
      "---------------------------  ----------  --------------\n",
      "SGDClassifier                      -inf             inf\n",
      "LogisticRegression                 -inf             inf\n",
      "Perceptron                         -inf             inf\n",
      "PassiveAggressiveClassifier        -inf             inf\n",
      "MLPClassifier                      -inf             inf\n",
      "KMeans                             -inf             inf\n",
      "KNeighborsClassifier               -inf             inf\n",
      "NearestCentroid                    -inf             inf\n",
      "RadiusNeighborsClassifier          -inf             inf\n",
      "SVC                                -inf             inf\n",
      "NuSVC                              -inf             inf\n",
      "LinearSVC                          -inf             inf\n",
      "RandomForestClassifier             -inf             inf\n",
      "DecisionTreeClassifier             -inf             inf\n",
      "ExtraTreesClassifier               -inf             inf\n",
      "============================================================\n",
      "The winner is: SGDClassifier with score -inf.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<hunga_bunga.classification.HungaBungaClassifier at 0x7fa8690a6208>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from hunga_bunga import HungaBungaRegressor, HungaBungaClassifier\n",
    "from core.models.metrics import avg_gain_ratio\n",
    "\n",
    "def f_scoring(model, x, y):\n",
    "    return avg_gain_ratio(y, model.predict(x))\n",
    "\n",
    "model = HungaBungaClassifier(brain=True, scoring=f_scoring, verbose=True)\n",
    "model.fit(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFittedError",
     "evalue": "This LinearRegression instance is not fitted yet. Call 'fit' with appropriate arguments before using this method.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFittedError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-e3b4c39db5a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxTrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/thesis/lib/python3.6/site-packages/hunga_bunga/regression.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 317\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    318\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/thesis/lib/python3.6/site-packages/sklearn/linear_model/base.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    219\u001b[0m             \u001b[0mReturns\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m         \"\"\"\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_decision_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0m_preprocess_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstaticmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_preprocess_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/thesis/lib/python3.6/site-packages/sklearn/linear_model/base.py\u001b[0m in \u001b[0;36m_decision_function\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_decision_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"coef_\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'csr'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'csc'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'coo'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/thesis/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_is_fitted\u001b[0;34m(estimator, attributes, msg, all_or_any)\u001b[0m\n\u001b[1;32m    912\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    913\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mall_or_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mattr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mattributes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 914\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mNotFittedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'name'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    915\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotFittedError\u001b[0m: This LinearRegression instance is not fitted yet. Call 'fit' with appropriate arguments before using this method."
     ]
    }
   ],
   "source": [
    "model.predict(xTrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loss function**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Metrics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.models.metrics import gain_mean, avg_gain_ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model: No data Augmentation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from core.utils.data_augmentation import DACombine, DASampling\n",
    "from core.models.deep import KerasModel, loss_tf, gain_tf\n",
    "import keras.backend as K\n",
    "\n",
    "model = KerasModel(loss=loss_tf, metrics=[gain_tf], batch_size=256, epochs=1000)\n",
    "da = DACombine()\n",
    "xTrain_a, yTrain_a = xTrain.astype(K.floatx()), yTrain.astype(K.floatx())\n",
    "split = int(xTrain.shape[1] * 0.75)\n",
    "#xTrain_a, yTrain_a = xTrain[:split], yTrain[:split]\n",
    "xVal, yVal = xTrain[split:], yTrain[split:]\n",
    "#xTrain_a, yTrain_a = da.fit_predict(xTrain, yTrain, size=1024, distance=10)\n",
    "history = model.fit(xTrain_a.astype(K.floatx()), yTrain_a.astype(K.floatx()),  validation_split=0.25, verbose=0)\n",
    "\n",
    "#print(history.history)\n",
    "\n",
    "loss_hist = pd.DataFrame(data={'loss': history.history['loss'], 'val_loss': history.history['val_loss']})\n",
    "loss_hist.plot()\n",
    "\n",
    "acc_hist = pd.DataFrame(data={'acc': history.history['gain_tf'], 'val_acc': history.history['val_gain_tf']})\n",
    "acc_hist.plot()\n",
    "\n",
    "yPred = model.predict(xTest, batch_size=128)\n",
    "\n",
    "out_data = pd.DataFrame(data={'y_test': np.ravel(yTest), 'y_pred': np.ravel(yPred)})\n",
    "#stl = model.score(xTest, yTest, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "yPred = model.predict(xTest)\n",
    "\n",
    "out_data = pd.DataFrame(data={'y_test': np.ravel(yTest), 'y_pred': np.ravel(yPred)})\n",
    "out_data.plot()\n",
    "\n",
    "yPred = model.predict(xTest)\n",
    "print(\"gain_mean: \", gain_mean(yTest.ravel(), yPred.ravel()))\n",
    "print(\"gain_ratio: \", avg_gain_ratio(yTest.ravel(), yPred.ravel()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model with data augmentation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# #sci-kit like training\n",
    "# model = KerasModel(loss=loss_tf, metrics=[gain_tf], batch_size=30, epochs=200)\n",
    "# da = DACombine()\n",
    "# xTrain_a, yTrain_a = xTrain, yTrain\n",
    "# split = int(xTrain.shape[1] * 0.75)\n",
    "# xTrain_a, yTrain_a = xTrain[:split], yTrain[:split]\n",
    "# xVal, yVal = xTrain[split:], yTrain[split:]\n",
    "# xTrain_a, yTrain_a = da.fit_predict(xTrain_a, yTrain_a, size=xTrain_a.shape[1]*16, distance=10, retarget=True, distribution=True, combine=True)\n",
    "# history = model.fit(xTrain_a.astype('float'), yTrain_a.astype('float'),  validation_data=(xVal, yVal), verbose=0)\n",
    "# loss_hist = pd.DataFrame(data={'loss': history.history['loss'], 'val_loss': history.history['val_loss']})\n",
    "# loss_hist.plot()\n",
    "\n",
    "# acc_hist = pd.DataFrame(data={'acc': history.history['gain_tf'], 'val_acc': history.history['val_gain_tf']})\n",
    "# acc_hist.plot()\n",
    "\n",
    "# yPred = model.predict(xTest, batch_size=128)\n",
    "\n",
    "# out_data = pd.DataFrame(data={'y_test': np.ravel(yTest), 'y_pred': np.ravel(yPred)})\n",
    "# #stl = model.score(xTest, yTest, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# yPred = model.predict(xTest)\n",
    "\n",
    "# out_data = pd.DataFrame(data={'y_test': np.ravel(yTest), 'y_pred': np.ravel(yPred)})\n",
    "# out_data.plot()\n",
    "\n",
    "# yPred = model.predict(xTest)\n",
    "# print(\"gain_mean: \", gain_mean(yTest.ravel(), yPred.ravel()))\n",
    "# print(\"gain_ratio: \", avg_gain_ratio(yTest.ravel(), yPred.ravel()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Model using _mse_ loss and data augmentation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# #sci-kit like training\n",
    "# model = KerasModel(loss='mse', metrics=[gain_tf], batch_size=60, epochs=200)\n",
    "# da = DACombine()\n",
    "# split = int(xTrain.shape[1] * 0.75)\n",
    "# xTrain_a, yTrain_a = xTrain[:split], yTrain[:split]\n",
    "# xVal, yVal = xTrain[split:], yTrain[split:]\n",
    "# #xTrain_a, yTrain_a = da.fit_predict(xTrain_a, yTrain_a, size=xTrain_a.shape[1]*16, distance=10, retarget=True, distribution=True, combine=True)\n",
    "# history = model.fit(xTrain_a.astype('float'), yTrain_a.astype('float'), validation_data=(xVal, yVal))\n",
    "# loss_hist = pd.DataFrame(data={'loss': history.history['loss'], 'val_loss': history.history['val_loss']})\n",
    "# loss_hist.plot()\n",
    "\n",
    "# acc_hist = pd.DataFrame(data={'acc': history.history['gain_tf'], 'val_acc': history.history['val_gain_tf']})\n",
    "# acc_hist.plot()\n",
    "\n",
    "# yPred = model.predict(xTest, batch_size=128)\n",
    "\n",
    "# out_data = pd.DataFrame(data={'y_test': np.ravel(yTest), 'y_pred': np.ravel(yPred)})\n",
    "# #stl = model.score(xTest, yTest, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# yPred = model.predict(xTest)\n",
    "\n",
    "# out_data = pd.DataFrame(data={'y_test': np.ravel(yTest), 'y_pred': np.ravel(yPred)})\n",
    "# out_data.plot()\n",
    "\n",
    "# yPred = model.predict(xTest)\n",
    "# print(\"gain_mean: \", gain_mean(yTest.ravel(), yPred.ravel()))\n",
    "# print(\"gain_ratio: \", avg_gain_ratio(yTest.ravel(), yPred.ravel()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.utils.data_augmentation import DASampling, DACombine\n",
    "from core.utils.preprocessing import df_to_xy, df_to_xydf\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "da = DACombine()\n",
    "\n",
    "#scaler = StandardScaler()\n",
    "#scaler.fit(xTrain)\n",
    "#xTrain = scaler.transform(xTrain)\n",
    "split = int(xTrain.shape[0] * 0.75)\n",
    "xTrain_a, yTrain_a = xTrain[:split], yTrain[:split]\n",
    "xVal, yVal = xTrain[split:], yTrain[split:]\n",
    "#da2 = D\n",
    "das = DASampling()\n",
    "\n",
    "xTrain_a, yTrain_a = da.fit_predict(xTrain_a, yTrain_a, size=10000, distance=5, retarget=True, distribution=True, combine=True)\n",
    "print(np.unique(yTrain_a))\n",
    "#xTrain_a, yTrain_a = das.generate_data(xTrain, yTrain, size=10000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.models.oracle import OracleModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.utils.benchmark import process_benchmark_cv\n",
    "\n",
    "model = OracleModel()\n",
    "\n",
    "# process_benchmark_cv(model, xTrain, yTrain)\n",
    "model.fit(xTrain, yTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.utils.benchmark import process_benchmarks\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.svm import LinearSVR, LinearSVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from core.models.acceptance import AcceptanceModel\n",
    "from core.models.featureless import EMModel, RandomModel\n",
    "from core.models.cluster import ClusterExtModel\n",
    "\n",
    "MODELS = {\n",
    "    \"random\": RandomModel(),\n",
    "    \"svr\": LinearSVR(),\n",
    "    \"em\": EMModel(),\n",
    "#     \"pagg\": PassiveAggressiveClassifier(),\n",
    "    \"forest\": RandomForestClassifier(n_estimators=100),\n",
    "    \"cluster\": ClusterExtModel(base_model=\"affinity\"),\n",
    "    \"mlp\": KerasModel(),\n",
    "}\n",
    "\n",
    "benchmark_models = {}\n",
    "for key_orac, orac in MODELS.items():\n",
    "    for key_mod, mod in MODELS.items():\n",
    "        benchmark_models[f\"oracle_{key_orac}_{key_mod}\"] = OracleModel(orac, mod)\n",
    "        \n",
    "# benchmark_models = {\n",
    "#     \"oracle_em_forest\": OracleModel(RandomModel(), RandomForestClassifier(n_estimators=100)),\n",
    "#     \"oracle_svr_svr\": OracleModel(LinearSVR(), LinearSVR()),\n",
    "#     \"oracle_forest_forest\": OracleModel(RandomForestClassifier(n_estimators=100), RandomForestClassifier(n_estimators=100)),\n",
    "#     \"oracle_pagg_pagg\": OracleModel(PassiveAggressiveClassifier(), PassiveAggressiveClassifier()),\n",
    "#     \"oracle_forest_pagg\": OracleModel(RandomForestClassifier(n_estimators=100), PassiveAggressiveClassifier()),\n",
    "#     \"oracle_forest_cluster\": OracleModel(RandomForestClassifier(n_estimators=100), ClusterExtModel(base_model=\"affinity\")),\n",
    "# }\n",
    "\n",
    "results = dict()\n",
    "results = process_benchmarks(benchmark_models, x, y.ravel(), augment_data=[None])\n",
    "\n",
    "results_mean = {key: item.mean() for key, item in results.items()}\n",
    "results_std = {key: item.std() for key, item in results.items()}\n",
    "results_df = pd.DataFrame(results_mean).T\n",
    "results_df.sort_values(\"avg_loss_ratio\", inplace=True)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.DataFrame(results_std).T * 100/ pd.DataFrame(results_mean).T\n",
    "m = ClusterExtModel(base_model=\"affinity\")\n",
    "m.fit(xTrain, yTrain)\n",
    "np.unique(m.predict(xTest)), np.unique(yTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.utils import to_categorical\n",
    "from keras import optimizers\n",
    "NN_model = Sequential()\n",
    "\n",
    "# The Input Layer :\n",
    "NN_model.add(Dense(128, kernel_initializer='normal',input_dim = xTrain.shape[1], activation='relu'))\n",
    "\n",
    "# The Hidden Layers :\n",
    "NN_model.add(Dense(256, kernel_initializer='normal',activation='relu'))\n",
    "NN_model.add(Dense(256, kernel_initializer='normal',activation='relu'))\n",
    "NN_model.add(Dense(256, kernel_initializer='normal',activation='relu'))\n",
    "\n",
    "# The Output Layer :\n",
    "NN_model.add(Dense(1, kernel_initializer='normal',activation='linear'))\n",
    "\n",
    "# Compile the network :\n",
    "NN_model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizers.RMSprop(lr=0.001), metrics=['acc'])\n",
    "NN_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# yPred = np.argmax(NN_model.predict(xTest), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.svm import LinearSVR, LinearSVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from core.models.acceptance import AcceptanceModel\n",
    "\n",
    "\n",
    "orac = MLPClassifier((1000,))\n",
    "#orac = LinearSVC()\n",
    "#mod = MLPClassifier((1000,))\n",
    "#orac = mod = NN_model\n",
    "orac = LinearSVC()\n",
    "mod = AcceptanceModel()\n",
    "\n",
    "model = OracleModel(mod, orac)\n",
    "model.fit(xTrain, yTrain.reshape(-1, 1))\n",
    "\n",
    "yPred = model.predict(xTest)\n",
    "\n",
    "m2 = AcceptanceModel.get_trained_model(xTrain, yTrain)\n",
    "yPred = m2.predict(xTrain)\n",
    "print(\"acc: \", avg_gain_ratio(yTrain, yPred))\n",
    "yPred = m2.predict(xTest)\n",
    "print(\"val_acc: \", avg_gain_ratio(yTest, yPred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CMP Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.svm import LinearSVR\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from core.models.acceptance import AcceptanceModel\n",
    "oracle = RandomForestClassifier(n_estimators=20, n_jobs=-1, max_depth=32)\n",
    "#oracle = MLPClassifier()\n",
    "#oracle = LinearSVR()\n",
    "#oracle.fit(xTrain_a, yTrain_a.ravel())\n",
    "#oracle = AcceptanceModel.get_trained_model(xTrain=xTrain_a, yTrain=yTrain_a.ravel(), epochs=3)\n",
    "#oracle = LinearSVR()\n",
    "oracle.fit(xTrain_a, yTrain_a.ravel())\n",
    "#oracle.partial_fit(xTrain, yTrain)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yPred = oracle.predict(xTrain_a)\n",
    "print(np.unique(yPred))\n",
    "print(\"train acc: \", gain_mean(yTrain_a, yPred))\n",
    "print((yPred==yTrain_a).sum())\n",
    "print(\"acc: \", avg_gain_ratio(yTrain, oracle.predict(xTrain)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xTrain_o = xTrain_a.copy()\n",
    "yTrain_o = oracle.predict(xTrain_o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(yTrain_o.astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# history = NN_model.fit(xTrain_o, to_categorical(yTrain_o, 200), epochs=200, shuffle=True, validation_split=0.2, batch_size=1024, verbose=0)\n",
    "# NN_model.compile(loss='categorical_crossentropy', optimizer=optimizers.RMSprop(lr=0.00001), metrics=['acc'])\n",
    "# history2 = NN_model.fit(xTrain, to_categorical(yTrain, 200), epochs=3000, validation_data=(xVal, to_categorical(yVal, 200)), shuffle=True, batch_size=128, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.models.deep import KerasModel\n",
    "from core.models.acceptance import AcceptanceModel\n",
    "from sklearn.linear_model import (LogisticRegression, LogisticRegressionCV, LinearRegression, ARDRegression,\n",
    "                                  ElasticNet, ElasticNetCV)\n",
    "from sklearn.linear_model import *\n",
    "from sklearn.svm import *\n",
    "from sklearn.model_selection import cross_val_score, cross_validate\n",
    "\n",
    "#model = KerasModel()\n",
    "# model.fit(xTrain_o, yTrain_o.ravel().astype(int), batch_size=512)\n",
    "#model = AcceptanceModel()\n",
    "#model = RandomForestClassifier(n_estimators=100, n_jobs=-1)\n",
    "#model = LinearSVR()\n",
    "#model.fit(xTrain_o, yTrain_o.ravel())\n",
    "#print(history.history)\n",
    "model = LinearRegression()\n",
    "# model = SVC(gamma=\"auto\")\n",
    "\n",
    "def avg_gain_score(estimator, x, y):\n",
    "    ypred = estimator.predict(x)\n",
    "    return avg_gain_ratio(y, ypred)\n",
    "\n",
    "model = LogisticRegression(penalty='l1', solver='liblinear', multi_class='auto')\n",
    "\n",
    "cv_res = cross_validate(model, xTrain, yTrain.ravel(), scoring=avg_gain_score,  cv=5, return_train_score=True, return_estimator=True)\n",
    "cv_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in cv_res[\"estimator\"]:\n",
    "    yPred = model.predict(xTest)\n",
    "    print(\"acc: \", avg_gain_ratio(yTest, yPred))\n",
    "# loss_hist = pd.DataFrame(data={'loss': history.history['loss'], 'val_loss': history.history['val_loss']})\n",
    "# loss_hist.plot()\n",
    "\n",
    "# acc_hist = pd.DataFrame(data={'acc': history.history['acc'], 'val_acc': history.history['val_acc']})\n",
    "# acc_hist.plot()\n",
    "\n",
    "# loss_hist = pd.DataFrame(data={'loss': history2.history['loss'], 'val_loss': history2.history['val_loss']})\n",
    "# loss_hist.plot()\n",
    "\n",
    "# acc_hist = pd.DataFrame(data={'acc': history2.history['acc'], 'val_acc': history2.history['val_acc']})\n",
    "# acc_hist.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression()\n",
    "parameters = {\n",
    "    'penalty': ('l1', 'elasticnet', 'l2', 'none'),\n",
    "    'dual': (False, True),\n",
    "    'solver': ('lbfgs', 'newton-cg', 'liblinear', 'sag', 'saga' ),\n",
    "    'multi_class': ('ovr', 'multinomial', 'auto'),\n",
    "    'max_iter': (100, 500, 1000),\n",
    "    'class_weight': ('balanced', None),\n",
    "    'C':[1, 10]\n",
    "}\n",
    "grid = GridSearchCV(model, parameters, cv=3, n_jobs=-1, scoring=avg_gain_score, return_train_score=True, error_score=0.0)\n",
    "grid.fit(xTrain, yTrain)\n",
    "print(grid.cv_results_[\"mean_test_score\"].max())\n",
    "\n",
    "yPred = grid.predict(xTest)\n",
    "print(np.unique(yPred))\n",
    "avg_gain_ratio(yTest, yPred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model = RandomForestClassifier()\n",
    "parameters = {\n",
    "    'n_estimators': (10, 100, 500),\n",
    "    'criterion': (\"gini\", \"entropy\"),\n",
    "    'max_depth': (None, 8, 32),\n",
    "    'min_samples_split': (None, 2, 16),\n",
    "    'min_samples_leaf': (None, 1, 16),\n",
    "    'max_features': (\"auto\", None),\n",
    "}\n",
    "grid = GridSearchCV(model, parameters, cv=3, n_jobs=-1, scoring=avg_gain_score, return_train_score=True, error_score=0.0)\n",
    "grid.fit(xTrain[:200], yTrain[:200])\n",
    "\n",
    "print(grid.cv_results_[\"mean_test_score\"].max())\n",
    "\n",
    "yPred = grid.predict(xTest)\n",
    "print(np.unique(yPred))\n",
    "avg_gain_ratio(yTest, yPred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import GridSearchCV\n",
    "# from sklearn.svm import LinearSVR\n",
    "# model = LinearSVR()\n",
    "# parameters = {\n",
    "#     'loss': ('epsilon_insensitive', 'squared_epsilon_insensitive'),\n",
    "#     'fit_intercept': (False, True),\n",
    "# #     'solver': ('lbfgs', 'newton-cg', 'liblinear', 'sag', 'saga' ),\n",
    "# #     'multi_class': ('ovr', 'multinomial', 'auto'),\n",
    "#     'max_iter': (100, 500, 1000),\n",
    "# #     'class_weight': ('balanced', None),\n",
    "#     'C':[1, 10]\n",
    "# }\n",
    "# grid = GridSearchCV(model, parameters, cv=3, n_jobs=-1, scoring=avg_gain_score, return_train_score=True, error_score=0.0)\n",
    "# grid.fit(xTrain, yTrain)\n",
    "# print(grid.cv_results_[\"mean_test_score\"].max())\n",
    "\n",
    "# yPred = grid.predict(xTest)\n",
    "# print(np.unique(yPred))\n",
    "# avg_gain_ratio(yTest, yPred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import GridSearchCV\n",
    "# from sklearn.svm import LinearSVC\n",
    "# model = LinearSVC()\n",
    "# parameters = {\n",
    "#     'penalty': ('l1', 'l2'),\n",
    "#     'loss': ('hinge', 'squared_hinge'),\n",
    "#     'dual': (True, False),\n",
    "# #     'shrinking': (True, False),\n",
    "# #     'solver': ('lbfgs', 'newton-cg', 'liblinear', 'sag', 'saga' ),\n",
    "#     'multi_class': ('ovr', 'crammer_singer'),\n",
    "#     'max_iter': (-1, 500, 1000),\n",
    "# #     'class_weight': ({i*5: 1 for i in range(41)},)\n",
    "# }\n",
    "# grid = GridSearchCV(model, parameters, cv=3, n_jobs=-1, scoring=avg_gain_score, return_train_score=True, error_score=0.0)\n",
    "# grid.fit(xTrain, yTrain)\n",
    "# print(grid.cv_results_[\"mean_test_score\"].max())\n",
    "\n",
    "# yPred = grid.predict(xTest)\n",
    "# print(np.unique(yPred))\n",
    "# avg_gain_ratio(yTest, yPred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import GridSearchCV\n",
    "# from sklearn.neural_network import MLPClassifier\n",
    "# model = MLPClassifier()\n",
    "# parameters = {\n",
    "#     'hidden_layer_sizes': (50, 100, 500),\n",
    "#     'activation': ('logistic', 'tanh', 'relu'),\n",
    "#     'solver': ('lbfgs', 'sgd', 'adam'),\n",
    "#     'batch_size': (64, 128, \"auto\"),\n",
    "#     'learning_rate': ('constant', 'invscaling', 'adaptive'),\n",
    "#     'max_iter': (100, 200, 500),\n",
    "# #     'kernel':('linear', 'rbf'),\n",
    "# }\n",
    "# grid = GridSearchCV(model, parameters, cv=3, n_jobs=-1, scoring=avg_gain_score, return_train_score=True, error_score=0.0)\n",
    "# grid.fit(xTrain, yTrain)\n",
    "# grid.cv_results_[\"mean_test_score\"].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KerasModel(batch_size=512)\n",
    "model.fit(xTrain_a, yTrain_a)\n",
    "yPred = model.predict(xTest)\n",
    "avg_gain_ratio(yTest, yPred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.fit(xTrain, yTrain, epochs=300)\n",
    "yPred = model.predict(xTest)\n",
    "print(np.unique(yPred))\n",
    "avg_gain_ratio(yTest, yPred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.get_params(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yPred = model.predict(xTest)\n",
    "gain_mean(yTest, yPred)\n",
    "# m = MLPClassifier()\n",
    "# m.fit(xTrain_o, yTrain_o.ravel())\n",
    "# m.partial_fit(xTrain, yTrain.ravel())\n",
    "# yPred = m.predict(xVal)\n",
    "# gain_mean(yVal, yPred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.layers import Dense, Dropout, Embedding, LSTM\n",
    "# from keras.models import Sequential\n",
    "# model = Sequential()\n",
    "# model.add(Embedding(input_dim=1000, output_dim=128, input_length=10))\n",
    "# model.add(LSTM(units=64))\n",
    "# model.add(Dropout(rate=0.5))\n",
    "# model.add(Dense(1, activation='sigmoid'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# model.compile(loss='mse', optimizer=optimizers.RMSprop(lr=0.001), metrics=['mse'])\n",
    "# model.fit(xTrain_a, yTrain_a, batch_size=1024, epochs=200)\n",
    "# model.compile(loss='mse', optimizer=optimizers.RMSprop(lr=0.00001), metrics=['mse'])\n",
    "# model.fit(xTrain, yTrain, validation_data=(xVal, yVal), epochs=100, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yPred = model.predict(xTest)\n",
    "gain_mean(yTest, yPred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(yPred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "m = np.random.random([12, 10])\n",
    "m.mean(), m.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##### from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "# from sklearn.ensemble import RandomForestClassifier, BaggingClassifier, VotingClassifier\n",
    "# from sklearn.pipeline import Pipeline\n",
    "# from sklearn.svm import LinearSVR, LinearSVC\n",
    "# from sklearn.linear_model import (ARDRegression, PassiveAggressiveClassifier, PassiveAggressiveRegressor,\n",
    "#                                   LogisticRegression, LogisticRegressionCV, SGDClassifier, SGDRegressor,\n",
    "#                                  TheilSenRegressor)\n",
    "# from sklearn.neural_network import MLPClassifier\n",
    "# from sklearn.neighbors import *\n",
    "# from sklearn.decomposition import KernelPCA, MiniBatchSparsePCA, FastICA\n",
    "# from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# from core.models import ConservativeModel, RandomModel, EMModel\n",
    "# pol = PolynomialFeatures(degree=3, include_bias=True, interaction_only=False)\n",
    "\n",
    "# pol.fit_transform(xTrain).shape\n",
    "\n",
    "# clf = Pipeline(\n",
    "#     [\n",
    "#         (\"poly\", PolynomialFeatures(degree=3, include_bias=False, interaction_only=False)),\n",
    "#         (\"scaler\", StandardScaler()),\n",
    "#         #(\"pca\", FastICA(8)),\n",
    "#         #(\"forest\", RandomForestClassifier(n_estimators=100, n_jobs=-1)),\n",
    "#         #(\"rnd\", RandomModel()),\n",
    "#         #(\"conservative\", ConservativeModel()),\n",
    "#         #(\"em\", EMModel()),\n",
    "#         #(\"ard\", ARDRegression()),\n",
    "#         #(\"sgd\", SGDClassifier(loss=\"epsilon_insensitive\", penalty=\"l1\")),\n",
    "#         #(\"clf\", RadiusNeighborsClassifier()),\n",
    "#         #(\"bag\", BaggingClassifier(n_jobs=-1)),\n",
    "#         #(\"linear_svr\", LinearSVR()),\n",
    "#         #(\"mlp\", MLPClassifier(hidden_layer_sizes=(1000, ))),\n",
    "#         (\"voting\", VotingClassifier([\n",
    "#             (\"mlp\", MLPClassifier()),\n",
    "#             (\"bag\", BaggingClassifier()),\n",
    "#             (\"forest\", RandomForestClassifier(n_estimators=100,)),\n",
    "#             (\"svc\", LinearSVC()),\n",
    "#             (\"sgd\", SGDClassifier()),\n",
    "#             (\"passiv\", PassiveAggressiveClassifier())\n",
    "#         ], n_jobs=-1))\n",
    "        \n",
    "#     ]\n",
    "# )\n",
    "\n",
    "# clf.fit(xTrain_a, yTrain_a.ravel())\n",
    "# yPred = clf.predict(xTrain)\n",
    "# print(\"train acc\", avg_gain_ratio(yTrain, yPred))\n",
    "# yPred = clf.predict(xTest)\n",
    "# print(\"val acc\", avg_gain_ratio(yTest, yPred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
