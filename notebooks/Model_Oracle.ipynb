{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from IPython.display import display\n",
    "from sklearn.model_selection import train_test_split, cross_validate\n",
    "from sklearn.metrics import r2_score\n",
    "import seaborn as sb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.utils.preprocessing import df_to_xy\n",
    "#fix rnd seed\n",
    "#np.random.seed(7)\n",
    "\n",
    "# Read and sanitize the data\n",
    "df = pd.read_excel(\"../data/t00/data.xls\")\n",
    "\n",
    "x, y = df_to_xy(df, fuse_risk=True, normalize=True, df_min=df.min(), df_max=df.max())\n",
    "xTrain, xTest, yTrain, yTest = train_test_split(x, y, test_size = 2/5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.21052632, 0.24324324, 0.26315789, 1.        , 0.05928854,\n",
       "       1.        , 0.76666658, 0.51351348, 0.39393936, 0.14734101])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loss function**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Metrics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.models.metrics import gain_mean, avg_gain_ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model: No data Augmentation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# model = KerasModel(loss=loss_tf, metrics=[gain_tf], batch_size=256, epochs=1000)\n",
    "# da = DACombine()\n",
    "# xTrain_a, yTrain_a = xTrain.astype(K.floatx()), yTrain.astype(K.floatx())\n",
    "# split = int(xTrain.shape[1] * 0.75)\n",
    "# #xTrain_a, yTrain_a = xTrain[:split], yTrain[:split]\n",
    "# xVal, yVal = xTrain[split:], yTrain[split:]\n",
    "# #xTrain_a, yTrain_a = da.fit_predict(xTrain, yTrain, size=1024, distance=10)\n",
    "# history = model.fit(xTrain_a.astype(K.floatx()), yTrain_a.astype(K.floatx()),  validation_split=0.25, verbose=0)\n",
    "\n",
    "# #print(history.history)\n",
    "\n",
    "# loss_hist = pd.DataFrame(data={'loss': history.history['loss'], 'val_loss': history.history['val_loss']})\n",
    "# loss_hist.plot()\n",
    "\n",
    "# acc_hist = pd.DataFrame(data={'acc': history.history['gain_tf'], 'val_acc': history.history['val_gain_tf']})\n",
    "# acc_hist.plot()\n",
    "\n",
    "# yPred = model.predict(xTest, batch_size=128)\n",
    "\n",
    "# out_data = pd.DataFrame(data={'y_test': np.ravel(yTest), 'y_pred': np.ravel(yPred)})\n",
    "# #stl = model.score(xTest, yTest, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# yPred = model.predict(xTest)\n",
    "\n",
    "# out_data = pd.DataFrame(data={'y_test': np.ravel(yTest), 'y_pred': np.ravel(yPred)})\n",
    "# out_data.plot()\n",
    "\n",
    "# yPred = model.predict(xTest)\n",
    "# print(\"gain_mean: \", gain_mean(yTest.ravel(), yPred.ravel()))\n",
    "# print(\"gain_ratio: \", avg_gain_ratio(yTest.ravel(), yPred.ravel()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model with data augmentation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# #sci-kit like training\n",
    "# model = KerasModel(loss=loss_tf, metrics=[gain_tf], batch_size=30, epochs=200)\n",
    "# da = DACombine()\n",
    "# xTrain_a, yTrain_a = xTrain, yTrain\n",
    "# split = int(xTrain.shape[1] * 0.75)\n",
    "# xTrain_a, yTrain_a = xTrain[:split], yTrain[:split]\n",
    "# xVal, yVal = xTrain[split:], yTrain[split:]\n",
    "# xTrain_a, yTrain_a = da.fit_predict(xTrain_a, yTrain_a, size=xTrain_a.shape[1]*16, distance=10, retarget=True, distribution=True, combine=True)\n",
    "# history = model.fit(xTrain_a.astype('float'), yTrain_a.astype('float'),  validation_data=(xVal, yVal), verbose=0)\n",
    "# loss_hist = pd.DataFrame(data={'loss': history.history['loss'], 'val_loss': history.history['val_loss']})\n",
    "# loss_hist.plot()\n",
    "\n",
    "# acc_hist = pd.DataFrame(data={'acc': history.history['gain_tf'], 'val_acc': history.history['val_gain_tf']})\n",
    "# acc_hist.plot()\n",
    "\n",
    "# yPred = model.predict(xTest, batch_size=128)\n",
    "\n",
    "# out_data = pd.DataFrame(data={'y_test': np.ravel(yTest), 'y_pred': np.ravel(yPred)})\n",
    "# #stl = model.score(xTest, yTest, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# yPred = model.predict(xTest)\n",
    "\n",
    "# out_data = pd.DataFrame(data={'y_test': np.ravel(yTest), 'y_pred': np.ravel(yPred)})\n",
    "# out_data.plot()\n",
    "\n",
    "# yPred = model.predict(xTest)\n",
    "# print(\"gain_mean: \", gain_mean(yTest.ravel(), yPred.ravel()))\n",
    "# print(\"gain_ratio: \", avg_gain_ratio(yTest.ravel(), yPred.ravel()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Model using _mse_ loss and data augmentation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# #sci-kit like training\n",
    "# model = KerasModel(loss='mse', metrics=[gain_tf], batch_size=60, epochs=200)\n",
    "# da = DACombine()\n",
    "# split = int(xTrain.shape[1] * 0.75)\n",
    "# xTrain_a, yTrain_a = xTrain[:split], yTrain[:split]\n",
    "# xVal, yVal = xTrain[split:], yTrain[split:]\n",
    "# #xTrain_a, yTrain_a = da.fit_predict(xTrain_a, yTrain_a, size=xTrain_a.shape[1]*16, distance=10, retarget=True, distribution=True, combine=True)\n",
    "# history = model.fit(xTrain_a.astype('float'), yTrain_a.astype('float'), validation_data=(xVal, yVal))\n",
    "# loss_hist = pd.DataFrame(data={'loss': history.history['loss'], 'val_loss': history.history['val_loss']})\n",
    "# loss_hist.plot()\n",
    "\n",
    "# acc_hist = pd.DataFrame(data={'acc': history.history['gain_tf'], 'val_acc': history.history['val_gain_tf']})\n",
    "# acc_hist.plot()\n",
    "\n",
    "# yPred = model.predict(xTest, batch_size=128)\n",
    "\n",
    "# out_data = pd.DataFrame(data={'y_test': np.ravel(yTest), 'y_pred': np.ravel(yPred)})\n",
    "# #stl = model.score(xTest, yTest, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# yPred = model.predict(xTest)\n",
    "\n",
    "# out_data = pd.DataFrame(data={'y_test': np.ravel(yTest), 'y_pred': np.ravel(yPred)})\n",
    "# out_data.plot()\n",
    "\n",
    "# yPred = model.predict(xTest)\n",
    "# print(\"gain_mean: \", gain_mean(yTest.ravel(), yPred.ravel()))\n",
    "# print(\"gain_ratio: \", avg_gain_ratio(yTest.ravel(), yPred.ravel()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0  10  15  40  45  55  60  65  75  80  85  90  95 100 105 110 125 130\n",
      " 155]\n"
     ]
    }
   ],
   "source": [
    "from core.utils.data_augmentation import DASampling, DACombine\n",
    "from core.utils.preprocessing import df_to_xy, df_to_xydf\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "da = DACombine()\n",
    "\n",
    "#scaler = StandardScaler()\n",
    "#scaler.fit(xTrain)\n",
    "#xTrain = scaler.transform(xTrain)\n",
    "split = int(xTrain.shape[0] * 0.75)\n",
    "xTrain_a, yTrain_a = xTrain[:split], yTrain[:split]\n",
    "xVal, yVal = xTrain[split:], yTrain[split:]\n",
    "#da2 = D\n",
    "das = DASampling()\n",
    "\n",
    "xTrain_a, yTrain_a = da.fit_predict(xTrain_a, yTrain_a, size=10000, distance=5, retarget=True, distribution=True, combine=True)\n",
    "print(np.unique(yTrain_a))\n",
    "#xTrain_a, yTrain_a = das.generate_data(xTrain, yTrain, size=10000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.models.oracle import OracleModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/franck/anaconda3/envs/thesis/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:921: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/franck/anaconda3/envs/thesis/lib/python3.6/site-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "/home/franck/Desktop/MA-Thesis/code/core/models/oracle.py:62: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  self.model.fit(xTrain_a, yTrain_a)\n"
     ]
    }
   ],
   "source": [
    "from core.utils.benchmark import process_benchmark_cv\n",
    "\n",
    "model = OracleModel()\n",
    "\n",
    "# process_benchmark_cv(model, xTrain, yTrain)\n",
    "OracleModel().fit(xTrain, yTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`classes=array([  0,   5,  10,  11,  14,  15,  18,  20,  22,  24,  25,  26,  27,\n",
      "        28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,\n",
      "        41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,\n",
      "        54,  55,  56,  57,  58,  59,  60,  61,  63,  64,  65,  66,  67,\n",
      "        69,  70,  71,  74,  75,  80,  85,  90,  95, 100, 105, 120, 125,\n",
      "       140, 150, 195])` is not the same as on last call to partial_fit, was: array([  5,  10,  40,  50,  70,  75,  80,  85,  90, 100, 105, 120, 125,\n",
      "       150])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "classes should include all valid labels that can be in y",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-4741a627ce25>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_benchmarks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbenchmark_models\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maugment_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0mresults_mean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/MA-Thesis/code/core/utils/benchmark.py\u001b[0m in \u001b[0;36mprocess_benchmarks\u001b[0;34m(models_dict, X, y, cv, fit_kwargs, predict_kwargs, augment_data, shuffle, metrics)\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0mxTest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myTest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodels_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m                 \u001b[0mbenchmark_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxTrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myTrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxTest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myTest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfit_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredict_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m                 \u001b[0mnKey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0maugment_data_step\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/MA-Thesis/code/core/utils/benchmark.py\u001b[0m in \u001b[0;36mprocess_model\u001b[0;34m(model, xTrain, yTrain, xTest, yTest, fit_kwargs, predict_kwargs, metrics)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mfit_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mfit_kwargs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mfit_kwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mpredict_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpredict_kwargs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mpredict_kwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxTrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myTrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0myPredict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxTest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpredict_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myTest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myPredict\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/MA-Thesis/code/core/models/oracle.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, xTrain, yTrain, model_fit_kwargs, oracle_fit_kwargs, oracle_split, size)\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartial_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxTrain_a\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myTrain_a\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartial_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxTrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myTrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/thesis/lib/python3.6/site-packages/sklearn/linear_model/passive_aggressive.py\u001b[0m in \u001b[0;36mpartial_fit\u001b[0;34m(self, X, y, classes)\u001b[0m\n\u001b[1;32m    222\u001b[0m                                  \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"hinge\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m                                  \u001b[0mclasses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m                                  coef_init=None, intercept_init=None)\n\u001b[0m\u001b[1;32m    225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoef_init\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mintercept_init\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/thesis/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py\u001b[0m in \u001b[0;36m_partial_fit\u001b[0;34m(self, X, y, alpha, C, loss, learning_rate, max_iter, classes, sample_weight, coef_init, intercept_init)\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;31m# Allocate datastructures from input arguments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         self._expanded_class_weight = compute_class_weight(self.class_weight,\n\u001b[0;32m--> 490\u001b[0;31m                                                            self.classes_, y)\n\u001b[0m\u001b[1;32m    491\u001b[0m         \u001b[0msample_weight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_sample_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/thesis/lib/python3.6/site-packages/sklearn/utils/class_weight.py\u001b[0m in \u001b[0;36mcompute_class_weight\u001b[0;34m(class_weight, classes, y)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         raise ValueError(\"classes should include all valid labels that can \"\n\u001b[0m\u001b[1;32m     42\u001b[0m                          \"be in y\")\n\u001b[1;32m     43\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mclass_weight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: classes should include all valid labels that can be in y"
     ]
    }
   ],
   "source": [
    "from core.utils.benchmark import process_benchmarks\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.svm import LinearSVR, LinearSVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from core.models.acceptance import AcceptanceModel\n",
    "from core.models.featureless import EMModel, RandomModel\n",
    "from core.models.cluster import ClusterExtModel\n",
    "\n",
    "MODELS = {\n",
    "    \"random\": RandomModel(),\n",
    "    \"svr\": LinearSVR(),\n",
    "    \"pagg\": PassiveAggressiveClassifier(),\n",
    "    \"forest\": RandomForestClassifier(n_estimators=100),\n",
    "    \"cluster\": ClusterExtModel(base_model=\"affinity\")\n",
    "}\n",
    "\n",
    "benchmark_models = {}\n",
    "for key_orac, orac in MODELS.items():\n",
    "    for key_mod, mod in MODELS.items():\n",
    "        benchmark_models[f\"oracle_{key_orac}_{key_mod}\"] = OracleModel(orac, mod)\n",
    "        \n",
    "# benchmark_models = {\n",
    "#     \"oracle_em_forest\": OracleModel(RandomModel(), RandomForestClassifier(n_estimators=100)),\n",
    "#     \"oracle_svr_svr\": OracleModel(LinearSVR(), LinearSVR()),\n",
    "#     \"oracle_forest_forest\": OracleModel(RandomForestClassifier(n_estimators=100), RandomForestClassifier(n_estimators=100)),\n",
    "#     \"oracle_pagg_pagg\": OracleModel(PassiveAggressiveClassifier(), PassiveAggressiveClassifier()),\n",
    "#     \"oracle_forest_pagg\": OracleModel(RandomForestClassifier(n_estimators=100), PassiveAggressiveClassifier()),\n",
    "#     \"oracle_forest_cluster\": OracleModel(RandomForestClassifier(n_estimators=100), ClusterExtModel(base_model=\"affinity\")),\n",
    "# }\n",
    "\n",
    "results = dict()\n",
    "results = process_benchmarks(benchmark_models, x, y.ravel(), augment_data=[None])\n",
    "\n",
    "results_mean = {key: item.mean() for key, item in results.items()}\n",
    "results_std = {key: item.std() for key, item in results.items()}\n",
    "results_df = pd.DataFrame(results_mean).T\n",
    "results_df.sort_values(\"avg_loss_ratio\", inplace=True)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 75.,  80., 100., 105.]),\n",
       " array([ 10,  15,  20,  25,  30,  40,  50,  60,  70,  75,  80,  90,  95,\n",
       "        100, 110, 120, 125, 140, 150, 195]))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pd.DataFrame(results_std).T * 100/ pd.DataFrame(results_mean).T\n",
    "m = ClusterExtModel(base_model=\"affinity\")\n",
    "m.fit(xTrain, yTrain)\n",
    "np.unique(m.predict(xTest)), np.unique(yTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.utils import to_categorical\n",
    "from keras import optimizers\n",
    "NN_model = Sequential()\n",
    "\n",
    "# The Input Layer :\n",
    "NN_model.add(Dense(128, kernel_initializer='normal',input_dim = xTrain.shape[1], activation='relu'))\n",
    "\n",
    "# The Hidden Layers :\n",
    "NN_model.add(Dense(256, kernel_initializer='normal',activation='relu'))\n",
    "NN_model.add(Dense(256, kernel_initializer='normal',activation='relu'))\n",
    "NN_model.add(Dense(256, kernel_initializer='normal',activation='relu'))\n",
    "\n",
    "# The Output Layer :\n",
    "NN_model.add(Dense(1, kernel_initializer='normal',activation='linear'))\n",
    "\n",
    "# Compile the network :\n",
    "NN_model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizers.RMSprop(lr=0.001), metrics=['acc'])\n",
    "NN_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# yPred = np.argmax(NN_model.predict(xTest), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.svm import LinearSVR, LinearSVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from core.models.acceptance import AcceptanceModel\n",
    "\n",
    "\n",
    "orac = MLPClassifier((1000,))\n",
    "#orac = LinearSVC()\n",
    "#mod = MLPClassifier((1000,))\n",
    "#orac = mod = NN_model\n",
    "orac = LinearSVC()\n",
    "mod = AcceptanceModel()\n",
    "\n",
    "model = OracleModel(mod, orac)\n",
    "model.fit(xTrain, yTrain.reshape(-1, 1))\n",
    "\n",
    "yPred = model.predict(xTest)\n",
    "\n",
    "m2 = AcceptanceModel.get_trained_model(xTrain, yTrain)\n",
    "yPred = m2.predict(xTrain)\n",
    "print(\"acc: \", avg_gain_ratio(yTrain, yPred))\n",
    "yPred = m2.predict(xTest)\n",
    "print(\"val_acc: \", avg_gain_ratio(yTest, yPred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CMP Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.svm import LinearSVR\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from core.models.acceptance import AcceptanceModel\n",
    "oracle = RandomForestClassifier(n_estimators=20, n_jobs=-1, max_depth=32)\n",
    "#oracle = MLPClassifier()\n",
    "#oracle = LinearSVR()\n",
    "#oracle.fit(xTrain_a, yTrain_a.ravel())\n",
    "#oracle = AcceptanceModel.get_trained_model(xTrain=xTrain_a, yTrain=yTrain_a.ravel(), epochs=3)\n",
    "#oracle = LinearSVR()\n",
    "oracle.fit(xTrain_a, yTrain_a.ravel())\n",
    "#oracle.partial_fit(xTrain, yTrain)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yPred = oracle.predict(xTrain_a)\n",
    "print(np.unique(yPred))\n",
    "print(\"train acc: \", gain_mean(yTrain_a, yPred))\n",
    "print((yPred==yTrain_a).sum())\n",
    "print(\"acc: \", avg_gain_ratio(yTrain, oracle.predict(xTrain)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xTrain_o = xTrain_a.copy()\n",
    "yTrain_o = oracle.predict(xTrain_o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(yTrain_o.astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# history = NN_model.fit(xTrain_o, to_categorical(yTrain_o, 200), epochs=200, shuffle=True, validation_split=0.2, batch_size=1024, verbose=0)\n",
    "# NN_model.compile(loss='categorical_crossentropy', optimizer=optimizers.RMSprop(lr=0.00001), metrics=['acc'])\n",
    "# history2 = NN_model.fit(xTrain, to_categorical(yTrain, 200), epochs=3000, validation_data=(xVal, to_categorical(yVal, 200)), shuffle=True, batch_size=128, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.models.deep import KerasModel\n",
    "from core.models.acceptance import AcceptanceModel\n",
    "from sklearn.linear_model import (LogisticRegression, LogisticRegressionCV, LinearRegression, ARDRegression,\n",
    "                                  ElasticNet, ElasticNetCV)\n",
    "from sklearn.linear_model import *\n",
    "from sklearn.svm import *\n",
    "from sklearn.model_selection import cross_val_score, cross_validate\n",
    "\n",
    "#model = KerasModel()\n",
    "# model.fit(xTrain_o, yTrain_o.ravel().astype(int), batch_size=512)\n",
    "#model = AcceptanceModel()\n",
    "#model = RandomForestClassifier(n_estimators=100, n_jobs=-1)\n",
    "#model = LinearSVR()\n",
    "#model.fit(xTrain_o, yTrain_o.ravel())\n",
    "#print(history.history)\n",
    "model = LinearRegression()\n",
    "# model = SVC(gamma=\"auto\")\n",
    "\n",
    "def avg_gain_score(estimator, x, y):\n",
    "    ypred = estimator.predict(x)\n",
    "    return avg_gain_ratio(y, ypred)\n",
    "\n",
    "model = LogisticRegression(penalty='l1', solver='liblinear', multi_class='auto')\n",
    "\n",
    "cv_res = cross_validate(model, xTrain, yTrain.ravel(), scoring=avg_gain_score,  cv=5, return_train_score=True, return_estimator=True)\n",
    "cv_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in cv_res[\"estimator\"]:\n",
    "    yPred = model.predict(xTest)\n",
    "    print(\"acc: \", avg_gain_ratio(yTest, yPred))\n",
    "# loss_hist = pd.DataFrame(data={'loss': history.history['loss'], 'val_loss': history.history['val_loss']})\n",
    "# loss_hist.plot()\n",
    "\n",
    "# acc_hist = pd.DataFrame(data={'acc': history.history['acc'], 'val_acc': history.history['val_acc']})\n",
    "# acc_hist.plot()\n",
    "\n",
    "# loss_hist = pd.DataFrame(data={'loss': history2.history['loss'], 'val_loss': history2.history['val_loss']})\n",
    "# loss_hist.plot()\n",
    "\n",
    "# acc_hist = pd.DataFrame(data={'acc': history2.history['acc'], 'val_acc': history2.history['val_acc']})\n",
    "# acc_hist.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression()\n",
    "parameters = {\n",
    "    'penalty': ('l1', 'elasticnet', 'l2', 'none'),\n",
    "    'dual': (False, True),\n",
    "    'solver': ('lbfgs', 'newton-cg', 'liblinear', 'sag', 'saga' ),\n",
    "    'multi_class': ('ovr', 'multinomial', 'auto'),\n",
    "    'max_iter': (100, 500, 1000),\n",
    "    'class_weight': ('balanced', None),\n",
    "    'C':[1, 10]\n",
    "}\n",
    "grid = GridSearchCV(model, parameters, cv=3, n_jobs=-1, scoring=avg_gain_score, return_train_score=True, error_score=0.0)\n",
    "grid.fit(xTrain, yTrain)\n",
    "print(grid.cv_results_[\"mean_test_score\"].max())\n",
    "\n",
    "yPred = grid.predict(xTest)\n",
    "print(np.unique(yPred))\n",
    "avg_gain_ratio(yTest, yPred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model = RandomForestClassifier()\n",
    "parameters = {\n",
    "    'n_estimators': (10, 100, 500),\n",
    "    'criterion': (\"gini\", \"entropy\"),\n",
    "    'max_depth': (None, 8, 32),\n",
    "    'min_samples_split': (None, 2, 16),\n",
    "    'min_samples_leaf': (None, 1, 16),\n",
    "    'max_features': (\"auto\", None),\n",
    "}\n",
    "grid = GridSearchCV(model, parameters, cv=3, n_jobs=-1, scoring=avg_gain_score, return_train_score=True, error_score=0.0)\n",
    "grid.fit(xTrain[:200], yTrain[:200])\n",
    "\n",
    "print(grid.cv_results_[\"mean_test_score\"].max())\n",
    "\n",
    "yPred = grid.predict(xTest)\n",
    "print(np.unique(yPred))\n",
    "avg_gain_ratio(yTest, yPred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import GridSearchCV\n",
    "# from sklearn.svm import LinearSVR\n",
    "# model = LinearSVR()\n",
    "# parameters = {\n",
    "#     'loss': ('epsilon_insensitive', 'squared_epsilon_insensitive'),\n",
    "#     'fit_intercept': (False, True),\n",
    "# #     'solver': ('lbfgs', 'newton-cg', 'liblinear', 'sag', 'saga' ),\n",
    "# #     'multi_class': ('ovr', 'multinomial', 'auto'),\n",
    "#     'max_iter': (100, 500, 1000),\n",
    "# #     'class_weight': ('balanced', None),\n",
    "#     'C':[1, 10]\n",
    "# }\n",
    "# grid = GridSearchCV(model, parameters, cv=3, n_jobs=-1, scoring=avg_gain_score, return_train_score=True, error_score=0.0)\n",
    "# grid.fit(xTrain, yTrain)\n",
    "# print(grid.cv_results_[\"mean_test_score\"].max())\n",
    "\n",
    "# yPred = grid.predict(xTest)\n",
    "# print(np.unique(yPred))\n",
    "# avg_gain_ratio(yTest, yPred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import GridSearchCV\n",
    "# from sklearn.svm import LinearSVC\n",
    "# model = LinearSVC()\n",
    "# parameters = {\n",
    "#     'penalty': ('l1', 'l2'),\n",
    "#     'loss': ('hinge', 'squared_hinge'),\n",
    "#     'dual': (True, False),\n",
    "# #     'shrinking': (True, False),\n",
    "# #     'solver': ('lbfgs', 'newton-cg', 'liblinear', 'sag', 'saga' ),\n",
    "#     'multi_class': ('ovr', 'crammer_singer'),\n",
    "#     'max_iter': (-1, 500, 1000),\n",
    "# #     'class_weight': ({i*5: 1 for i in range(41)},)\n",
    "# }\n",
    "# grid = GridSearchCV(model, parameters, cv=3, n_jobs=-1, scoring=avg_gain_score, return_train_score=True, error_score=0.0)\n",
    "# grid.fit(xTrain, yTrain)\n",
    "# print(grid.cv_results_[\"mean_test_score\"].max())\n",
    "\n",
    "# yPred = grid.predict(xTest)\n",
    "# print(np.unique(yPred))\n",
    "# avg_gain_ratio(yTest, yPred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import GridSearchCV\n",
    "# from sklearn.neural_network import MLPClassifier\n",
    "# model = MLPClassifier()\n",
    "# parameters = {\n",
    "#     'hidden_layer_sizes': (50, 100, 500),\n",
    "#     'activation': ('logistic', 'tanh', 'relu'),\n",
    "#     'solver': ('lbfgs', 'sgd', 'adam'),\n",
    "#     'batch_size': (64, 128, \"auto\"),\n",
    "#     'learning_rate': ('constant', 'invscaling', 'adaptive'),\n",
    "#     'max_iter': (100, 200, 500),\n",
    "# #     'kernel':('linear', 'rbf'),\n",
    "# }\n",
    "# grid = GridSearchCV(model, parameters, cv=3, n_jobs=-1, scoring=avg_gain_score, return_train_score=True, error_score=0.0)\n",
    "# grid.fit(xTrain, yTrain)\n",
    "# grid.cv_results_[\"mean_test_score\"].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KerasModel(batch_size=512)\n",
    "model.fit(xTrain_a, yTrain_a)\n",
    "yPred = model.predict(xTest)\n",
    "avg_gain_ratio(yTest, yPred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.fit(xTrain, yTrain, epochs=300)\n",
    "yPred = model.predict(xTest)\n",
    "print(np.unique(yPred))\n",
    "avg_gain_ratio(yTest, yPred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.get_params(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yPred = model.predict(xTest)\n",
    "gain_mean(yTest, yPred)\n",
    "# m = MLPClassifier()\n",
    "# m.fit(xTrain_o, yTrain_o.ravel())\n",
    "# m.partial_fit(xTrain, yTrain.ravel())\n",
    "# yPred = m.predict(xVal)\n",
    "# gain_mean(yVal, yPred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.layers import Dense, Dropout, Embedding, LSTM\n",
    "# from keras.models import Sequential\n",
    "# model = Sequential()\n",
    "# model.add(Embedding(input_dim=1000, output_dim=128, input_length=10))\n",
    "# model.add(LSTM(units=64))\n",
    "# model.add(Dropout(rate=0.5))\n",
    "# model.add(Dense(1, activation='sigmoid'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# model.compile(loss='mse', optimizer=optimizers.RMSprop(lr=0.001), metrics=['mse'])\n",
    "# model.fit(xTrain_a, yTrain_a, batch_size=1024, epochs=200)\n",
    "# model.compile(loss='mse', optimizer=optimizers.RMSprop(lr=0.00001), metrics=['mse'])\n",
    "# model.fit(xTrain, yTrain, validation_data=(xVal, yVal), epochs=100, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yPred = model.predict(xTest)\n",
    "gain_mean(yTest, yPred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(yPred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "m = np.random.random([12, 10])\n",
    "m.mean(), m.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##### from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "# from sklearn.ensemble import RandomForestClassifier, BaggingClassifier, VotingClassifier\n",
    "# from sklearn.pipeline import Pipeline\n",
    "# from sklearn.svm import LinearSVR, LinearSVC\n",
    "# from sklearn.linear_model import (ARDRegression, PassiveAggressiveClassifier, PassiveAggressiveRegressor,\n",
    "#                                   LogisticRegression, LogisticRegressionCV, SGDClassifier, SGDRegressor,\n",
    "#                                  TheilSenRegressor)\n",
    "# from sklearn.neural_network import MLPClassifier\n",
    "# from sklearn.neighbors import *\n",
    "# from sklearn.decomposition import KernelPCA, MiniBatchSparsePCA, FastICA\n",
    "# from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# from core.models import ConservativeModel, RandomModel, EMModel\n",
    "# pol = PolynomialFeatures(degree=3, include_bias=True, interaction_only=False)\n",
    "\n",
    "# pol.fit_transform(xTrain).shape\n",
    "\n",
    "# clf = Pipeline(\n",
    "#     [\n",
    "#         (\"poly\", PolynomialFeatures(degree=3, include_bias=False, interaction_only=False)),\n",
    "#         (\"scaler\", StandardScaler()),\n",
    "#         #(\"pca\", FastICA(8)),\n",
    "#         #(\"forest\", RandomForestClassifier(n_estimators=100, n_jobs=-1)),\n",
    "#         #(\"rnd\", RandomModel()),\n",
    "#         #(\"conservative\", ConservativeModel()),\n",
    "#         #(\"em\", EMModel()),\n",
    "#         #(\"ard\", ARDRegression()),\n",
    "#         #(\"sgd\", SGDClassifier(loss=\"epsilon_insensitive\", penalty=\"l1\")),\n",
    "#         #(\"clf\", RadiusNeighborsClassifier()),\n",
    "#         #(\"bag\", BaggingClassifier(n_jobs=-1)),\n",
    "#         #(\"linear_svr\", LinearSVR()),\n",
    "#         #(\"mlp\", MLPClassifier(hidden_layer_sizes=(1000, ))),\n",
    "#         (\"voting\", VotingClassifier([\n",
    "#             (\"mlp\", MLPClassifier()),\n",
    "#             (\"bag\", BaggingClassifier()),\n",
    "#             (\"forest\", RandomForestClassifier(n_estimators=100,)),\n",
    "#             (\"svc\", LinearSVC()),\n",
    "#             (\"sgd\", SGDClassifier()),\n",
    "#             (\"passiv\", PassiveAggressiveClassifier())\n",
    "#         ], n_jobs=-1))\n",
    "        \n",
    "#     ]\n",
    "# )\n",
    "\n",
    "# clf.fit(xTrain_a, yTrain_a.ravel())\n",
    "# yPred = clf.predict(xTrain)\n",
    "# print(\"train acc\", avg_gain_ratio(yTrain, yPred))\n",
    "# yPred = clf.predict(xTest)\n",
    "# print(\"val acc\", avg_gain_ratio(yTest, yPred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
